{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the versions\n",
    "print(f'Tensorflow Versio: {tf.__version__}')\n",
    "print(f'Eager mode: {tf.executing_eagerly()}')\n",
    "print(f'Hub Version: {hub.__version__}')\n",
    "print(f'GPU is {\"availble\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download IMDB reviews dataset\n",
    "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\",\"test\"], batch_size=-1, as_supervised=True)\n",
    "train_features, train_labels = tfds.as_numpy(train_data) \n",
    "test_features, test_labels = tfds.as_numpy(test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data\n",
    "print(f'Training examples: {len(train_features)}, Test examples: {len(test_features)}')\n",
    "X = train_features[:10]\n",
    "y = train_labels[:10]\n",
    "for i in range(len(X)):\n",
    "    print(f'{y[i]}: {X[i]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In building a neural network model, we have to make 3 architectural decisions, namely:\n",
    "- How to represent the text?\n",
    "- How many layers to use in the model?\n",
    "- How many hidden units to use in each layer?\n",
    "\n",
    "In this particular example, input data represent texts and the labels are either 0 or 1.\n",
    "We can convert the texts into embeddings vector using pretrained text embeddings. There are 2 advantages associated with using pretrained text embeddings, namely:\n",
    "- No worries about preprocessing the texts\n",
    "- Exploit the benefits of transfer learning\n",
    "\n",
    "For this example, we can use pretrained embeddings model from Tensorflow hub called google/nnlm-en-dim50/2. There are two other models:\n",
    "- google/nnlm-en-dim50-with-normalization/2, same as the above named model but with normalization to remove punctuations, thereby improving in-vocabulary coverage.\n",
    "- google/nnlm-en-dim128-with-normalization/2, which uses embedding dimension of 128 instead of 50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create KerasLayer that uses Tensorflow Hub model to embed sentences, expected output of the layer is (number of examples, embedding dimension). In this case, the dimension is (3,50)\n",
    "# model = \"http://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "model = \"http://tfhub.dev/google/nnlm-en-dim50-with-normalization/2\"\n",
    "hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)\n",
    "hub_layer(train_features[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the full model\n",
    "# model uses a pretrained saved model to map sentences to the embedding vectors (split sentence to token, generates token embedding and then combine token embedding to vector of fixed size 50)\n",
    "# The fixed length vector is piped through a layer with 16 hidden units\n",
    "# The last layer is a single output node, with logits output.\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(32, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(16, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model, specifying th loss function and the optimizr for training. Since this is a binary classification problem, and the output is a probability, we'll use the binary_crossentropy loss function.\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name=\"accuracy\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation set\n",
    "validation_features = train_features[:10000]\n",
    "validation_labels = train_labels[:10000]\n",
    "\n",
    "train_features = train_features[10000:]\n",
    "train_labels = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = model.fit(train_features,\n",
    "                          train_labels,\n",
    "                          epochs=20,\n",
    "                          batch_size=512,\n",
    "                          validation_data=(validation_features, validation_labels),\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: plot the training and validation loss from the model training\n",
    "history = train_history.history\n",
    "accuracy = history['accuracy']\n",
    "validation_accuracy = history['val_accuracy']\n",
    "loss = history['loss']\n",
    "validation_loss = history['val_loss']\n",
    "\n",
    "epochs =  range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, loss, 'ro', label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: plot training and validation accuracy \n",
    "plt.clf() # clear figure to plot new data\n",
    "\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, validation_accuracy, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
